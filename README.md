Real-Time Security Anomaly Detection System
Overview
This project implements a real-time anomaly detection system using unsupervised machine learning models and AI-driven solutions. It integrates components such as data ingestion from Kafka, model training, anomaly detection, real-time alerting, object detection in CCTV footage using YOLO, and deployment to AWS Lambda or Google Cloud. The system monitors multiple security systems (e.g., network traffic, access logs, CCTV footage) to detect anomalies, providing an early warning system to identify potential security threats.
The project includes AutoML optimization using Optuna, real-time monitoring with Prometheus, batch processing for real-time anomaly detection, and a Flask-based dashboard for anomaly insights and performance metrics. Additionally, it integrates a Rasa chatbot for interactive anomaly querying.

System Workflow
1. Data Ingestion:
Real-time data ingestion from Kafka topics asynchronously.
Data security: Encryption applied to ensure sensitive information is secure.
Preprocessing: Normalizing data and applying dimensionality reduction using PCA.
2. Anomaly Detection:
AutoML optimization: Models such as Isolation Forest, KMeans, and Autoencoders are optimized using Optuna.
Batch processing: Real-time anomaly detection in continuous data streams.
Object detection: YOLO-based object detection is used in CCTV footage to identify suspicious objects or behaviors.
Real-time alerting: Detected anomalies trigger alerts through AWS SNS.
3. Model Deployment:
The system supports deployment of models to AWS Lambda and Google Cloud for scalability.
The system is capable of auto-scaling based on resource usage.
4. Monitoring:
Real-time monitoring of system performance and detected anomalies using Prometheus.
Flask-based dashboard to visualize system performance and anomalies.
5. Rasa Chatbot:
AI chatbot built using Rasa allows querying detected anomalies and interacting with the anomaly detection system.

Directory Structure
bash
Copy code
.
├── RealTimeSecurityAnomalyDetector.py    # Main Python script
├── download_yolo_model.py               # Helper script to download YOLO model files
├── generate_test_data.py                # Script to generate synthetic test data
├── run_tests.py                         # Script to run unit tests
├── deploy_lambda.sh                     # Shell script for AWS Lambda deployment
├── README.md                            # Documentation file (this file)
├── requirements.txt                     # Python dependencies
├── config.yaml                          # Configuration file (AWS, Kafka, model paths)
├── models/                              # Folder containing trained models and YOLO files
│   ├── coco.names                       # YOLO class names
│   ├── yolov3.cfg                       # YOLO configuration file
│   ├── yolov3.weights                   # YOLO weights file, available at https://www.kaggle.com/datasets/shivam316/yolov3-weights
└── logs/                                # Log files generated by the system


Installation Instructions
Requirements
To run the project, you'll need the following installed on your machine:
Python 3.8+
Kafka
Apache Spark
Redis
Docker
AWS CLI configured
Google Cloud CLI configured
Prometheus and Grafana for monitoring
Rasa for chatbot
Python Dependencies
To install the required Python packages, run:
bash
Copy code
pip install -r requirements.txt

Environment Variables
You must set the following environment variables in your system:
AWS_REGION: AWS region (e.g., us-east-1)
SNS_TOPIC_ARN: ARN for the AWS SNS topic used for sending alerts.
ENCRYPTION_KEY: Encryption key for encrypting and decrypting data.
Running the System
To start the anomaly detection system, simply run:
bash
Copy code
python RealTimeSecurityAnomalyDetector.py


Detailed Function Descriptions
1. process_cctv_with_yolo(video_path, frame_resize=(416, 416))
Purpose: Processes CCTV footage using YOLO object detection to identify suspicious objects or activities.
Parameters:
video_path (str): Path to the CCTV footage file.
frame_resize (tuple): Dimensions to resize the video frames for faster processing.
Input: Video file from CCTV.
Output: List of detected anomalies (e.g., "suspicious_object", "loitering").
2. init_spark_session(app_name="RealTimeSecurityAnomalyDetection")
Purpose: Initializes a Spark session for distributed data processing.
Parameters:
app_name (str): Name of the Spark application.
Output: A Spark session for data processing.
3. load_data_from_kafka(topic_name, bootstrap_servers='localhost:9092', retries=3)
Purpose: Asynchronously loads real-time data from a Kafka topic with retry logic.
Parameters:
topic_name (str): Kafka topic name to consume data from.
bootstrap_servers (str): Kafka broker server address.
retries (int): Number of retry attempts in case of a failed connection.
Input: Kafka stream.
Output: Yielded Pandas DataFrames with data from Kafka.
4. encrypt_data(data, encryption_key)
Purpose: Encrypts sensitive data using Fernet symmetric encryption.
Parameters:
data (str): Data to encrypt.
encryption_key (str): Encryption key for secure encryption.
Output: Encrypted data.
5. decrypt_data(encrypted_data, encryption_key)
Purpose: Decrypts encrypted data using Fernet symmetric encryption.
Parameters:
encrypted_data (str): Encrypted data to decrypt.
encryption_key (str): Encryption key for decryption.
Output: Decrypted data in string format.
6. preprocess_data(df, scaling_method='standard', pca_var=0.95)
Purpose: Preprocesses raw data by normalizing it and reducing dimensionality using PCA.
Parameters:
df (pd.DataFrame): Input data to preprocess.
scaling_method (str): Method for scaling ('standard' or 'minmax').
pca_var (float): Percentage of variance to retain when applying PCA.
Input: Raw data in Pandas DataFrame.
Output: Preprocessed data as a NumPy array.
7. extract_network_features(log_data)
Purpose: Extracts key network features such as packet size and data rate from logs.
Parameters:
log_data (pd.DataFrame): Network log data to extract features from.
Output: NumPy array of extracted features (packet size, data rate).
8. optimize_isolation_forest(trial)
Purpose: Optimizes Isolation Forest hyperparameters using Optuna for anomaly detection.
Parameters: trial: Optuna trial object for hyperparameter search.
Output: Optimized Isolation Forest model.
9. optimize_autoencoder(data, trial)
Purpose: Optimizes an autoencoder model for dimensionality reduction using Optuna.
Parameters:
data (np.array): Input data to train the autoencoder.
trial: Optuna trial object for hyperparameter search.
Output: Optimized autoencoder model.
10. run_automl(data)
Purpose: Runs AutoML to find the best model and hyperparameters using Optuna for anomaly detection.
Parameters: data (np.array): Input data for training the models.
Output: Best Optuna trial object containing the optimized model.
11. real_time_detection(model, data_stream, detection_type='isolation_forest', batch_size=10)
Purpose: Performs real-time anomaly detection with batch processing from a data stream.
Parameters:
model: Trained anomaly detection model.
data_stream: Stream of data to process.
detection_type (str): Type of model used for detection (e.g., 'isolation_forest').
batch_size (int): Number of data points to process in each batch.
Input: Streaming data.
Output: Detected anomalies and triggered alerts.
12. send_alert(message, retries=3)
Purpose: Sends an alert via AWS SNS with retry logic in case of failure.
Parameters:
message (str): Alert message content.
retries (int): Number of retries to send the alert if it fails.
Output: AWS SNS notification.
13. track_model_latency(start_time, data_points, model_name)
Purpose: Tracks and logs model inference latency and the number of data points processed.
Parameters:
start_time (float): Start time of model inference.
data_points (int): Number of data points processed.
model_name (str): Name of the model.
Output: Latency logged in seconds.
14. create_flask_dashboard()
Purpose: Creates a Flask-based dashboard to visualize real-time anomalies and system metrics.
Output: Flask app that serves the dashboard on localhost.
15. initialize_rasa_chatbot()
Purpose: Initializes the Rasa chatbot for querying the system about detected anomalies.
Output: Initialized Rasa chatbot agent.
16. handle_user_query(agent, user_input)
Purpose: Processes user queries using the Rasa chatbot and returns anomaly insights.
Parameters:
agent: Rasa chatbot agent.
user_input (str): User query to the chatbot.
Output: Response from the chatbot with relevant insights.

Helper Scripts
1. download_yolo_model.py
Purpose: Downloads the required YOLO model files (weights, config, class names).
Usage:
bash
Copy code
python download_yolo_model.py


2. generate_test_data.py
Purpose: Generates synthetic test data for testing the anomaly detection system.
Usage:
bash
Copy code
python generate_test_data.py


3. run_tests.py
Purpose: Runs unit tests to validate the system’s components.
Usage:
bash
Copy code
python run_tests.py


4. deploy_lambda.sh
Purpose: Deploys the trained model to AWS Lambda for serverless anomaly detection.
Usage:
bash
Copy code
./deploy_lambda.sh



Deployment Options
AWS Lambda Deployment:
Deploy your models to AWS Lambda for scalable, serverless execution:
python
Copy code
deploy_on_aws_lambda(model_path="path/to/your/model.zip")

Google Cloud Deployment:
Upload models to Google Cloud Storage for scalable inference:
python
Copy code
deploy_on_google_cloud(model_path="path/to/your/model.zip", bucket_name="your-bucket-name")


Monitoring & Alerts
Prometheus: Real-time performance metrics are tracked using Prometheus.
AWS SNS: Alerts are sent via AWS SNS when anomalies are detected.

Future Work
1. Advanced Anomaly Detection Models:
Incorporate GANs (Generative Adversarial Networks) to enhance anomaly detection by generating synthetic anomalies.
Use hybrid models combining unsupervised learning and deep learning for better detection accuracy.
2. Integration with Additional Data Sources:
Integrate more security data sources such as firewall logs, IoT device logs, and social media analysis for a broader security overview.
3. Improved Real-Time Processing:
Implement Apache Flink for more robust real-time data processing and lower latency in anomaly detection.
4. Expanded Model Deployment:
Add Azure Machine Learning deployment for enterprises using Azure-based infrastructure.
Explore edge computing for low-latency inference on local devices.
5. Model Interpretability and Explainability:
Use LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations) to explain model decisions and build trust with non-technical stakeholders.

Challenges and Solutions
1. High False Positives:
Challenge: Unsupervised models may generate too many false positives.
Solution: Implement feedback loops from security experts to adjust thresholds and tune the models. Post-processing steps like correlation analysis with historical events can help reduce false positives.
2. Scalability:
Challenge: Handling large amounts of security data in real-time requires high scalability.
Solution: Use Kubernetes to enable horizontal scaling and cloud-based deployments like AWS Lambda to dynamically manage computing resources.
3. Data Imbalance:
Challenge: Anomalies are rare in the data, leading to class imbalance problems.
Solution: Apply SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic anomalies during model training to handle data imbalance.
4. Latency in Real-Time Applications:
Challenge: In real-time security applications, latency in anomaly detection could delay responses to security threats.
Solution: Implement Apache Flink and in-memory databases like Redis to reduce latency.
5. Model Drift:
Challenge: Patterns of normal behavior change over time, which may cause the model to become less accurate (model drift).
Solution: Implement scheduled retraining using updated data, and continuously monitor performance to detect when retraining is necessary.
